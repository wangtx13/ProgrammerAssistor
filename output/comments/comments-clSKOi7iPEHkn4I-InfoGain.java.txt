xxx this is disgustingly d i s g u s t i n g l y non thread safe xxx yuck figure out how to remove this not strictly part of a list of feature info gains but convenient and efficient for ml classify decisiontree decision tree feature location value feature location index populate targetfeaturecount target feature count et al the below relies on labelweights label weights summing to 1 over all labels! xxx is this right? what should we do about negative values? whatever is decided here should also go in decisiontree decision tree split xxx should this instead by infinite? calculate the overall entropy of the labels ignoring the features out print targetcount target count vector print targetcount target count out targetcountsum target count sum = +targetcountsum +target count sum out total entropy = +staticbaseentropy +static base entropy calculate the infogain info gain of each feature alphabet dictionary = ilist getdataalphabet get data alphabet out feature= +dictionary lookupsymbol lookup symbol fi + presentweight= present weight= + featurecountsum feature count sum fi /targetcountsum /target count sum + absentweight= absent weight= + targetcountsum target count sum featurecountsum feature count sum fi /targetcountsum /target count sum + presententropy= present entropy= +featurepresententropy+ +feature present entropy+ absententropy= absent entropy= +featureabsententropy +feature absent entropy 