/* there are several different kinds of numeric values 

 weights range from inf to inf high weights make a path more
 likely these don't appear directly in transducer but appear
	 as to many subclasses such as crfs c r fs weights are also
	 often summed or combined in a dot product with feature vectors 

	 unnormalized costs range from inf to inf high costs make a
	 path less likely unnormalized costs can be obtained from negated
	 weights or negated sums of weights these are often by a
	 transitioniterator's transition iterator's getvalue get value the latticenode lattice node alpha
	 values are unnormalized costs 

	 normalized costs range from 0 to inf high costs make a path
	 less likely normalized costs can safely be considered as the
	 log probability of some event they can be obtained by
	 subtracting a negative normalizer from unnormalized costs for
	 example subtracting the total cost of a lattice typically
	  typically
	 initialcosts initial costs and finalcosts costs are examples of normalized costs but
	 they are also allowed to be unnormalized costs the gammas 
	 stategammas state gammas and transitionxis transition xis are all normalized costs as
	 well as the value of lattice getvalue get value 

	 probabilities range from 0 to 1 high probabilities make a path
	 more likely they are obtained from normalized costs by taking the
	 log and negating 

	 sums of probabilities range from 0 to positive numbers they are
	 the sum of several probabilities these are passed to the
	 incrementcount increment count 

 */ sparsevector sparse vector weights defaultweights default weights 	 for default feature alphabet weightalphabet weight alphabet = new alphabet weightsfrozen weights frozen featureinduction feature induction can fill this in featureselections feature selections is on a per weights i basis and over rides permanently disabling featureinducer's feature inducer's and setweightsdimensionsasin set weights dimensions as in from using these features on these transitions store here the induced feature conjunctions so that these conjunctions can be added to test instances before transduction an index that gets incremented each time this crfs c r fs get changed an index that gets incremented each time this crfs c r fs parameters' structure get changed a copy of weightsstructurechangestamp weights structure change stamp the last time numparameters num was calculated on transitions indexed by weight index for default features indexed by weight index flag if indicating that the weights of this weight index should not be changed by learning indexed by weight index indexed by state index indexed by state index leave the rest as they will get set later by addstate add state and addweight add weight alternatively we could create zero length arrays we don't copy here because we want expectation and constraint factors to get changes to a crf c r f factor alternatively we declare freezing to be a change of structure and force reallocation of expectations etc todo t o d o change this to this crf todo t o d o consider cloning this instead gsc checking each sparsevector's sparse vector's size within weights note that we are not checking the indices of the sparsevectors sparse vectors in weights gsc checks all weights to make sure there are no nan na n or infinite values this can be called for checking the weights of constraints and expectations but not for crf since it can have infinite weights associated with states that are not likely gsc checking initial/final weights of crf as well since we could have a state machine where some states have infinite initial and/or weight todo t o d o note that there doesn't seem to be a way to freeze the initialweights initial weights and finalweights weights todo t o d o should we also obey featureselection feature selection here? no need it is enforced by the creation of the weights todo t o d o this could use some careful checking over especially for flipped negations gsc checking initial/final weights of crf as well since we could have a state machine where some states have infinite initial and/or weight todo t o d o note that there doesn't seem to be a way to freeze the initialweights initial weights and finalweights weights todo t o d o should we also obey featureselection feature selection here? no need it is enforced by the creation of the weights for frozen weights don't even gather their sufficient statistics this is how we ensure that the gradient for these will be zero todo t o d o should we also obey featureselection feature selection here? no need it is enforced by the creation of the weights for frozen weights don't even gather their sufficient statistics this is how we ensure that the gradient for these will be zero todo t o d o should we also obey featureselection feature selection here? no need it is enforced by the creation of the weights gsc serialization for factors inputalphabet input alphabet stopgrowth stop growth xxx outputalphabet output alphabet stopgrowth stop growth this assumes that other has non inputpipe input pipe and outputpipe output pipe we'd need to add another constructor to handle this if not this will copy all the transition weights weightalphabet weight alphabet = alphabet initialcrf initial c r f weightalphabet weight alphabet clone weights = new sparsevector sparse vector initialcrf initial c r f weights length clear these because they will be filled by this addstate add state yyy weightsfrozen weights frozen = initialcrf initial c r f weightsfrozen weights frozen clone this can be over ridden in subclasses of crf c r f to subclasses of crf c r f state this is assuming the the entries in the outputalphabet output alphabet are strings! initialweight initial weight of 0 0 handle start state the half labels will include all observed tests the transition weights will include only the default feature a new empty featureselection feature selection won't allow any features here so we only get the default feature for transitions the half labels will include all observational tests the transition weights will include only the default feature a new empty featureselection feature selection won't allow any features here so we only get the default feature for transitions this is assuming the the entries in the outputalphabet output alphabet are strings! this is assuming the the entries in the outputalphabet output alphabet are strings! this is assuming the the entries in the outputalphabet output alphabet are strings! using empty feature selection gives us only the default features support for making cc mallet optimize optimizable crfs c r fs is this necessary? akm 11/2007 gsc changing this to consider the case when trainingdata training data is a mix of labeled and unlabeled data and we want to use the unlabeled data as well to set some weights while using the unsupported trick *note* 'target' sequence of an unlabeled instance is either or is of size zero the value doesn't actually change because the new will have zero value but the gradient changes because the now have different layout put in the weights that are already there put in the weights in the training set gsc trainingdata training data can have unlabeled instances as well do it for the paths consistent with the labels and also do it for the paths selected by the current model so we will get some negative weights do this once some training is done only create features for transitions with probability above 0 2 this 0 2 is somewhat arbitrary akm out crf4 c r f4 has index +indices j put in the previous weights respect the featureselection feature selection create a new weight vector if weightname weight name is new use initial capacity of 8 settrainable set trainable gsc accessor kedar access structure stamp gsc but it is here as a reminder to do something about inducefeaturesfor induce features for */ /*
		testing setfeatureselection set feature selection this globalfeatureselection global feature selection 
		for i = 0 i < featureinducers feature inducers size i++ {
			featureinducer {
			 feature inducer klfi = featureinducer feature inducer featureinducers feature inducers get i 
			klfi inducefeaturesfor induce features for testing 
		}
		eval evaluate this 0 0 0 testing 
		*/ todo t o d o put support to optimizable here including getvalue get value instancelist instance list ?? index = rfv indexatlocation index at location rfv getindexatrank get index at rank m this doesn't make any sense how did this ever work? akm 12/2007 gsc serialization for crf c r f why is this ? couldn't it be a non inner class? in transducer also akm 12/2007 indexed by destination state feature index n b elements are until getdestinationstate get destination state is called contains indices into crf c r f weights no arg constructor so serialization works note setting these here is actually redundant they were set already in crf c r f addstate add state i'm considering removing initialweight initial weight and finalweight weight as arguments to this constructor but need to think more akm 12/2007 if crf c r f state were non then this constructor could add the state to the list of states and put it in the name2state also make sure this label appears in our output alphabet to make it efficient inside incrementtransition increment transition serialization for state xxx or do we want output equals here? here is the dot product of the feature weights with the lambda weights for one transition include with implicit weight 1 0 the default feature prepare nextindex next index pointing at the next non impossible transition these s are just to try to make this more efficient perhaps some of them will have to go away serialization transitioniterator transition iterator break looping over features 