the training instances and their topic assignments the alphabet for the input data the alphabet for the topics the number of topics requested the size of the vocabulary prior dirichlet alpha alpha is the distribution over topics prior on per topic multinomial distribution over words an to put the topic counts for the current document initialized locally below defined here to avoid garbage collection overhead indexed by <document index topic index> statistics needed for sampling indexed by <feature index topic index> indexed by <topic index> loop over every document in the corpus occasionally print more information 		populate topic counts 	iterate 	 iterate over the positions words in the document grab the relevant row from our two dimensional 	remove 	 remove this token from all counts now calculate and add up the scores for each topic for this word here's where the math happens! note that overall performance is dominated by what you do in this loop choose a random point between 0 and the sum of all topic scores figure out which topic contains that point make sure we actually sampled a topic put that new topic into the counts the likelihood of the model is a combination of a dirichlet multinomial for the words in each topic and a dirichlet multinomial for the topics in each document the likelihood function of a dirichlet multinomial is 	 gamma sum_i alpha_i 	 prod_i gamma alpha_i + n_i 	prod_i gamma alpha_i 	 gamma sum_i alpha_i + n_i so the log likelihood is 	loggamma 	log gamma sum_i alpha_i loggamma log gamma sum_i alpha_i + n_i + 	 sum_i loggamma log gamma alpha_i + n_i loggamma log gamma alpha_i do the documents first subtract the count + parameter sum term add the parameter sum term and the topics reuse this as a pointer for displaying and saving results initialize the sorters with dummy values count up the tokens and normalize serialization instance lists 