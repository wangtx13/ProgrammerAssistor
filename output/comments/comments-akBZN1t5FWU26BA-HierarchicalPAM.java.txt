constants to determine the level of the output multinomial number of topics to be fit this parameter controls the balance between the local document counts and the global distribution over super topics this parameter is the smoothing on that global distribution and the same for sub topics prior on per topic multinomial distribution over words data the data field of the instances is expected to hold a featuresequence feature sequence gibbs sampling state these could be shorts or we could encode both in one indexed by <document index sequence index> indexed by <document index sequence index> per document state variables # of words per <super sub> # of words per <super> the component of the gibbs update that depends on super topics the component of the gibbs update that depends on sub topics unnormalized sampling distribution a cache of the cumulative weight for each super topic document frequencies used for minimal path hierarchical dirichlets and their sums note that this last is not the same as supertopicdocumentfrequencies super topic document frequencies cached priors per word type state variables indexed by <feature index topic index> indexed by <topic index> indexed by <topic index> we can't calculate betasum beta sum until we know how many word types allocate several arrays for use within each document to cut down memory allocation and garbage collection time initialize with random assignments of tokens to topics and finish allocating this topics and this tokens randomly assign tokens to topics random super topic random sub topic initialize cached priors finally start the sampler! loop over every word in the corpus indexed by seq position starttime start time = currenttimemillis current time millis populate topic counts iterate over the positions words in the document remove this token from all counts the document frequencies have changed decrement and recalculate the prior weights update the super topic weight for the old topic build a distribution over super sub topic pairs for this token the conditional probability of each super sub pair is proportional to an expression with three parts one that depends only on the super topic one that depends only on the sub topic and the word type and one that depends on the super sub pair sample a topic assignment from this distribution we picked the root topic go over the row sums to find the super topic now read across to find the sub topic go over each sub topic until the weight is less l e s s than the sample note that we're subtracting weights in the same order we added them save the choice into the gibbs state put the new super/sub topics into the counts update the weight for the new super topic the likelihood of the model is a combination of a dirichlet multinomial for the words in each topic and a dirichlet multinomial for the topics in each document the likelihood function of a dirichlet multinomial is gamma sum_i alpha_i prod_i gamma alpha_i + n_i prod_i gamma alpha_i gamma sum_i alpha_i + n_i so the log likelihood is loggamma log gamma sum_i alpha_i loggamma log gamma sum_i alpha_i + n_i + sum_i loggamma log gamma alpha_i + n_i loggamma log gamma alpha_i do the documents first account for words assigned to super topic the term for the sums account for words assigned to the root topic subtract the count + parameter sum term add the parameter sum term for every document all at once and the topics count the number of type topic pairs reuse this as a pointer 