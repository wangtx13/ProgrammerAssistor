expgain exp gain of a feature f is defined in terms of maxent max ent type feature+class feature s f f = f c expgain exp gain of a feature f is g f = kl k l p~ c ||q c kl k l p~ c ||q_f c where p~ is the empirical distribution according to the label distribution and q is the distribution from the imperfect classifier and q_f is the distribution from the imperfect classifier with f added and f's weight adjusted but none of the other weights adjusted expgain exp gain of a feature f is g f = sum_c g f c it would be more accurate to a gain number for each feature/class combination but here we simply the gain feature = \sum_{class} gain feature xxx not ever used remove them notation from della pietra lafferty 1997 p 4 p~ q alpha the weight of the new feature feature location index feature location value calculate p~ f and q f instanceweight instance weight = ilist getinstanceweight get instance weight i the below relies on labelweights label weights summing to 1 over all labels! if i < 500 out i= +i+ li= +li+ true= +truelabelweight+ +true label weight+ model= +modellabelweight +model label weight xxx note that we are not attenting to instanceweight instance weight here! p fli += truelabelweight label weight * instanceweight instance weight / numinstances+1 num instances+1 q fli += modellabelweight model label weight * instanceweight instance weight / numinstances+1 num instances+1 /*
		double psum = 0 
		double qsum = 0 
		for i = 0 i < numclasses num classes i++ 
			for j = 0 j < numfeatures num features j++ {
				psum += p i j 
				qsum += q i j 
			}
		assert math abs psum 1 0 < 0 0001 psum not 1 0! psum= +psum+ qsum= +qsum 
		assert math abs qsum 1 0 < 0 0001 qsum not 1 0! psum= +psum+ qsum= +qsum 
*/ determine the alphas we can't do it in closed form as in the della pietra paper because this we have here a conditional maxent max ent model so we do it by newton raphson initializing by the broken inappropriate joint case closed form solution for i = 0 i < numclasses num classes i++ for j = 0 j < numfeatures num features j++ alphas i j = math log p i j * 1 0 q i j / q i j * 1 0 p i j first derivative change in alpha last iteration change in alpha last iteration change in alpha last iteration second derivative xxx change to more? alphas are initialized to zero out newton iteration +newton /*usinghyperbolicprior*/ /*using hyperbolic prior*/ gaussian prior xxx this assumes binary valued features what about tied weights? we now now first and second derivative for this newton step run tests on the alphas and their derivatives and do a newton step print just a sampling of them 					assert !double ! isnan is na n alphas i j 					assert !double ! isnan is na n dalphas i j 					assert !double ! isnan is na n ddalphas i j xxx assert ddalphas i j <= 0 assert math abs alphachange < 100 0 alphachange xxx arbitrary? trying to prevent a cycle out updating alphamax alpha max +i+ +j+ = +alphas i j out updating alphamin alpha min +i+ +j+ = +alphas i j newton wants to jump to a point inside the boundaries let it newton wants to jump to a point outside the boundaries bisect instead out newton tried to exceed bounds bisecting dalphas +i+ +j+ = +dalphas i j + alphamin= alpha min= +alphamin +alpha min i j + alphamax= alpha max= +alphamax +alpha max i j allow some memory to be freed q = q e^{\alpha g} p 4 out calculating qeag note that we are using a gaussian prior so we don't multiply by 1/numinstances 1/num instances following line now done before outside of loop over instances for fi = 0 fi < numfeatures num features fi++ qeag fi += modellabelweight model label weight * 1 0 when the value of this feature g is zero a value of 1 0 should be included in the expectation we'll actually add all these at the end pre assuming that all features have value zero here we subtract the assumed modellabelweight model label weight and put in the value based on non zero valued feature g out calculating klgain values serialization 