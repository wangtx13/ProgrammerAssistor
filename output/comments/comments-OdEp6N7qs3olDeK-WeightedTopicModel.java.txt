the training instances and their topic assignments the alphabet for the input data the alphabet for the topics the number of topics requested the size of the vocabulary prior dirichlet alpha alpha is the distribution over topics an to put the topic counts for the current document initialized locally below defined here to avoid garbage collection overhead indexed by <document index topic index> statistics needed for sampling indexed by <feature index topic index> indexed by <topic index> weights on type type interactions complains if we don't initialize loop over every document in the corpus 			for doc = 0 doc < 5000 doc++ { run the sampler in initialization mode for the first iteration and show debugging info for the first document /*
				if doc+1 % 1000 == 0 {
					system {
					 out doc + 1 
				}
				*/ occasionally print more information 		populate topic counts 	iterate 	 iterate over the positions words in the document grab the relevant row from our two dimensional 	remove 	 remove this token from all counts already incremented now calculate and add up the scores for each topic for this word here's where the math happens! note that overall performance is dominated by what you do in this loop choose a random point between 0 and the sum of all topic scores figure out which topic contains that point make sure we actually sampled a topic /*
				system /*
				 out alphabet lookupobject lookup type 
				for topic = 0 topic < numtopics num topics topic++ {
					system {
					 out + alpha + + + localtopiccounts local topic counts topic + * +
									 + currenttypetopicweights current type topic weights topic + / + totaltopicweights total topic weights topic + = +
									 topictermscores topic term scores topic 
				}
				*/ throw new illegalstateexception illegal state weightedtopicmodel weighted topic model new topic not sampled put that new topic into the counts out newtopic new topic + \t + alphabet lookupobject lookup type already incremented /*
	public modelloglikelihood model log likelihood {
		double loglikelihood log likelihood = 0 0 
		int nonzerotopics non zero topics 

		 the likelihood of the model is a combination of a 
		 dirichlet multinomial for the words in each topic
		 and a dirichlet multinomial for the topics in each
		 document 

		 the likelihood function of a dirichlet multinomial is
		 	 gamma sum_i alpha_i 	 prod_i gamma alpha_i + n_i 
		 	prod_i gamma alpha_i 	 gamma sum_i alpha_i + n_i 

		 so the log likelihood is 
		 	loggamma 	log gamma sum_i alpha_i loggamma log gamma sum_i alpha_i + n_i + 
		 	 sum_i loggamma log gamma alpha_i + n_i loggamma log gamma alpha_i 

		 do the documents first

		int topiccounts topic counts = new numtopics num topics 
		double topicloggammas topic log gammas = new numtopics num topics 
		int doctopics doc topics 

		for topic=0 topic < numtopics num topics topic++ {
			topicloggammas {
			topic log gammas topic = dirichlet loggamma log gamma alpha 
		}
	
		for doc=0 doc < data size doc++ {
			labelsequence {
			 label sequence topicsequence topic sequence = labelsequence label sequence data get doc topicsequence topic sequence 

			doctopics 

			doc topics = topicsequence topic sequence getfeatures get features 

			for token=0 token < doctopics doc topics length token++ {
				topiccounts {
				topic counts doctopics doc topics token ++ 
			}

			for topic=0 topic < numtopics num topics topic++ {
				if topiccounts topic counts topic > 0 {
					loglikelihood {
					log likelihood += dirichlet loggamma log gamma alpha + topiccounts topic counts topic 
									 topicloggammas topic log gammas topic 
				}
			}

			 subtract the count + parameter sum term
			loglikelihood term
			log likelihood = dirichlet loggamma log gamma alphasum alpha sum + doctopics doc topics length 

			arrays 

			 arrays fill topiccounts topic counts 0 
		}
	
		 add the parameter sum term
		loglikelihood term
		log likelihood += data size * dirichlet loggamma log gamma alphasum alpha sum 

		 and the topics

		 count the number of type topic pairs
		int nonzerotypetopics non zero type topics = 0 

		for type=0 type < numtypes num types type++ {
			 reuse this as a pointer

			topiccounts pointer

			topic counts = typetopiccounts type topic counts type 

			for topic = 0 topic < numtopics num topics topic++ {
				if topiccounts topic counts topic == 0 { continue }
				
				nonzerotypetopics++ }
				
				non zero type topics++ 
				loglikelihood 
				log likelihood += dirichlet loggamma log gamma beta + topiccounts topic counts topic 

				if isnan is na n loglikelihood log likelihood {
					system {
					 out topiccounts topic counts topic 
					system 
					 exit 1 
				}
			}
		}
	
		for topic=0 topic < numtopics num topics topic++ {
			loglikelihood {
			log likelihood = 
				dirichlet 
				 dirichlet loggamma log gamma beta * numtopics num topics +
											tokenspertopic +
											tokens per topic topic 
			if isnan is na n loglikelihood log likelihood {
				system {
				 out after topic + topic + + tokenspertopic tokens per topic topic 
				system 
				 exit 1 
			}

		}
	
		loglikelihood 
			}

		}
	
		log likelihood += 
			 dirichlet loggamma log gamma beta * numtopics num topics 
			 dirichlet loggamma log gamma beta * nonzerotypetopics non zero type topics 

		if isnan is na n loglikelihood log likelihood {
			system {
			 out at the end 
			system 
			 exit 1 
		}


		return loglikelihood log likelihood 
	}
	*/ for displaying and saving results the type topic counts are dense meaning that the index of the element determines its topic the marginal estimator uses the sparse bit encoded arrays used in paralleltopicmodel parallel topic model so we need to convert to that format exact power of 2 otherwise add an extra bit first figure out how many entries we have allocate the sparse and fill it keeping the in descending order move values along note that arrays are all zeros at initialization we've now found where to insert push along any other values now add it to the of arrays for doc = 0 doc < 5000 doc++ { 