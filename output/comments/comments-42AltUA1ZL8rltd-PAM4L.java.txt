number of topics to be fit dirichlet alpha alpha is the distribution over supertopics prior on per topic multinomial distribution over words data the data field of the instances is expected to hold a featuresequence feature sequence gibbs sampling state these could be shorts or we could encode both in one indexed by <document index sequence index> indexed by <document index sequence index> per document state variables # of words per <super sub> # of words per <super> the component of the gibbs update that depends on super topics the component of the gibbs update that depends on sub topics unnormalized sampling distribution a cache of the cumulative weight for each super topic per word type state variables indexed by <feature index topic index> indexed by <topic index> for debugging purposes indexed by <topic index> histograms for mle m l e histogram of # of words per supertopic in documents eg 17 4 is # of docs with 4 words in st s t 17 for each supertopic histogram of # of words per subtopic initialize the sub topic alphas to a symmetric dirichlet we can't calculate vbeta v beta until we know how many word types 		allocate 		 allocate several arrays for use within each document 		to cut down memory allocation and garbage collection time 		initialize 		 initialize with random assignments of tokens to topics 		and finish allocating this topics and this tokens randomly assign tokens to topics random super topic random sub topic for the sub topic we also need to update the word type statistics 		these 		 these will be initialized at the first call to 		clearhistograms 		clear histograms in the loop below 		finally 		 finally start the sampler! there are a few things we do on round numbered iterations that don't make sense if this is the first iteration this write new outputmodelfilename+' output model filename+' '+iterations else out print 		124 5 seconds 		144 8 seconds after using featuresequence feature sequence instead of tokens 		121 6 seconds after putting on featuresequence feature sequence getindexatposition get index at position 		106 3 seconds after avoiding lookup in inner loop with a temporary variable 		initialize 		 initialize the parameter sum 		the 		 the histogram arrays go up to the size of the largest document 		but the non zero values will almost always cluster in the low end 		we 		 we avoid looping over empty arrays by saving the index of the largest 		non zero value calculate the denominator iterate over the histogram /*
 if isnan is na n denominator {
	system {
	 out parametersum parameter sum 
	for i=1 i < observationlengths observation lengths length i++ {
	 out print observationlengths observation lengths i + 
	}
	system 
	}
	 out 
 }
			 */ calculate the individual what's the largest non zero element in the histogram? if there are no tokens assigned to this super sub pair anywhere in the corpus bail /* one iteration of gibbs sampling across all documents */ 		loop 		 loop over every word in the corpus indexed by seq position 		long starttime start time = currenttimemillis current time millis 		populate topic counts 		iterate 		 iterate over the positions words in the document remove this token from all counts build a distribution over super sub topic pairs for this token clear the data structures avoid two layer ie accesses the conditional probability of each super sub pair is proportional to an expression with three parts one that depends only on the super topic one that depends only on the sub topic and the word type and one that depends on the super sub pair calculate each of the super only factors first next calculate the sub only factors finally put them together sample a topic assignment from this distribution go over the row sums to find the super topic now read across to find the sub topic go over each sub topic until the weight is less l e s s than the sample note that we're subtracting weights in the same order we added them save the choice into the gibbs state put the new super/sub topics into the counts 		update 		 update the topic count histograms 		for dirichlet estimation this looks broken dm d m populate per document topic counts print the subtopic prortions sorted print the supertopic prortions sorted /*
 write f {
try {
 objectoutputstream output stream oos = new objectoutputstream output stream new fileoutputstream output stream f 
 oos writeobject write this 
 oos close 
}
catch ioexception i o e {
 err writing + f + + e 
}
 }

 serialization

  serialization

 serialversionuid serial uid = 1 
 current_serial_version = 0 
 null_integer = 1 

 writeobject write objectoutputstream output stream out ioexception i o {
out writeint write current_serial_version 
out writeobject write ilist 
out writeint write numtopics num topics 
out writeobject write alpha 
out writedouble write beta 
out writedouble write vbeta v beta 
for di = 0 di < topics length di ++ 
 for si = 0 si < topics di length si++ 
	out writeint write topics di si 
for fi = 0 fi < numtypes num types fi++ 
 for ti = 0 ti < numtopics num topics ti++ 
	out writeint write typetopiccounts type topic counts fi ti 
for ti = 0 ti < numtopics num topics ti++ 
 out writeint write tokenspertopic tokens per topic ti 
 }

 readobject read objectinputstream input stream in ioexception i o classnotfoundexception not found {
int featureslength features length 
int = in readint read 
ilist = instancelist instance list in readobject read 
numtopics 
num topics = in readint read 
alpha = in readobject read 
beta = in readdouble read 
vbeta 
v beta = in readdouble read 
int numdocs num docs = ilist size 
topics = new numdocs num docs 
for di = 0 di < ilist size di++ {
 doclen doc len = featuresequence feature sequence ilist getinstance get instance di getdata get data getlength get length 
	 topics di = new doclen doc len 
	 for si = 0 si < doclen doc len si++ 
		topics di si = in readint read 
}

int numtypes num types = ilist getdataalphabet get data alphabet size 
typetopiccounts 
type topic counts = new numtypes num types numtopics num topics 
for fi = 0 fi < numtypes num types fi++ 
 for ti = 0 ti < numtopics num topics ti++ 
	typetopiccounts 
	type topic counts fi ti = in readint read 
tokenspertopic 
tokens per topic = new numtopics num topics 
for ti = 0 ti < numtopics num topics ti++ 
 tokenspertopic tokens per topic ti = in readint read 
 }
	 */ recommended to use mallet/bin/vectors2topics instead should be 1100 		pam printdocumenttopics print document topics new args 0 + pam 