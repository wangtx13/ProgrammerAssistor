these values are used to encode type/topic counts as count/topic pairs in a single exact power of 2 otherwise add an extra bit initialize all positions to the most common topic for that type ignore out of vocabulary terms this value should be a topic such that no other topic has more tokens of this type assigned to it if for some reason there were no tokens of this type in the training data it will default to topic 0 which is no worse than random initialization build an that densely lists the topics that														 have non zero counts 																					 record the total number of non zero topics																 	 initialize the topic count/beta sampling bucket													 initialize cached coefficients and the topic/beta														 normalizing constant 																					 initialize the normalization constant for the b * n_{t|d} term									 update the coefficients for the non zero topics													 iterate over the positions words in the document														 ignore out of vocabulary terms prepare to sample by adjusting existing counts note that we do not need to change the smoothing only mass since the denominator is clamped decrement the local doc/topic counts																 assert localtopiccounts local topic counts oldtopic old topic >= 0 maintain the dense index if we are deleting														 the old topic																						 first get to the dense location associated with													 the old topic 																					 we know it's in there somewhere so we don't													 need bounds checking 																			 shift all remaining dense indices to the left 													 finished maintaining local topic index reset the cached coefficient for this topic															 make sure it actually gets set																		 topictermcount++ topic term count++ 																				 betatopiccount++ beta topic count++ 																			 if this is a new topic for this document 															 add the topic to the dense index 																	 first find the point where we																	 should insert the new topic by going to														 the end which is the only reason we're keeping												 track of the number of non zero																 topics and working backwards																	 update the coefficients for the non zero topics													 save a sample clean up our mess reset the coefficients to values with only smoothing the next doc will update its own non zero topics save at least one sample normalize initialize the sorters with dummy values serialization 