allowing the default feature the base level to fluctuate more freely than the feature leads to much better results add one feature for the default feature add a spot for the intercept term out num features + numfeatures num features + numlabels num labels + numlabels num labels this constraints = new numlabels num labels * numfeatures num features initialize the constraints testmaximizable test maximizable testvalueandgradientcurrentparameters test value and gradient current this incorporate likelihood of data out l now +inputalphabet +input alphabet size + regular features get the predicted probability of each under the current model exponentiate the scores due to underflow it's very likely that some of these scores will be 0 0 this is really an but since featurevectors feature vectors are defined as doubles avoid casting checking out value incorporate prior on the log of a gaussian prior is x^2 / 2sigma^2 incorporate likelihood of data get the predicted probability of each under the current model exponentiate the scores due to underflow it's very likely that some of these scores will be 0 0 in a featurevector feature vector there's no easy way to say do you know about this id? so i've broken this into two for loops one for all labels the other for just the non zero ones now add the default feature a parameter may be set to infinity by an external user we set gradient to 0 because the parameter's value can never change anyway and it will mess up future calculations on the matrix such as norm out dcmmaxenttrainer d c m max ent trainer gradient infinity norm = +matrixops + matrix ops infinitynorm infinity norm cachedgradient cached gradient 