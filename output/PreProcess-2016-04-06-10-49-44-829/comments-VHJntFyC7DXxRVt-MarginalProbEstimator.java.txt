number of topics to be fit these values are used to encode type/topic counts as count/topic pairs in a single dirichlet alpha alpha is the distribution over topics prior on per topic multinomial distribution over words indexed by <feature index topic index> indexed by <topic index> exact power of 2 otherwise add an extra bit initialize the smoothing only sampling bucket initialize the cached coefficients using only smoothing these values will be selectively replaced in documents with non zero counts in particular topics keep track of the number of tokens we've examined not including out of vocabulary words build an that densely lists the topics that have non zero counts record the total number of non zero topics 		initialize 		 initialize the topic count/beta sampling bucket all counts are now zero we are starting completely fresh 	iterate 	 iterate over the positions words in the document record the marginal probability of the token at the current limit summed over all topics iterate up to the current limit check for out of vocabulary words 	remove 	 remove this token from all counts remove this topic's contribution to the normalizing constants note that we are using clamped estimates of p w|t so we are not n o t changing smoothingonlymass smoothing only mass decrement the local doc/topic counts maintain the dense index if we are deleting the old topic first get to the dense location associated with the old topic we know it's in there somewhere so we don't need bounds checking shift all remaining dense indices to the left add the old topic's contribution back into the normalizing constants reset the cached coefficient for this topic now go over the type/topic counts calculating the score for each topic 	make 	 make sure it actually gets set betatopiccount++ beta topic count++ smoothingonlycount++ smoothing only count++ todo t o d o is this appropriate throw new illegalstateexception illegal state workerrunnable worker runnable new topic not sampled assert newtopic new topic != 1 			put 			 put that new topic into the counts if this is a new topic for this document add the topic to the dense index first find the point where we should insert the new topic by going to the end which is the only reason we're keeping track of the number of non zero topics and working backwards 	update the coefficients for the non zero topics we've just resampled all tokens up u p to t o the current limit now sample the token at a t the current limit check for out of vocabulary words out + currenttopic current topic + = + currentvalue current value /* debugging to make sure we're getting the right probabilities
			 for topic = 0 topic < numtopics num topics topic++ {
			 index = 0 
			 displaycount display count = 0 
				
			 while index < currenttypetopiccounts current type topic counts length 
			 currenttypetopiccounts current type topic counts index > 0 {
			 currenttopic current topic = currenttypetopiccounts current type topic counts index topicmask topic mask 
			 currentvalue current value = currenttypetopiccounts current type topic counts index >> topicbits topic bits 

			 if currenttopic current topic == topic {
			 displaycount display count = currentvalue current value 
			 break 
			 }

			 index++ 
			 }
				
			 out print topic + \t 
			 out print + localtopiccounts local topic counts topic + + + alpha topic + / +
			 + alphasum alpha sum + + + tokenssofar tokens so far + * 

			 out + displaycount display count + + + beta + / +
			 + tokenspertopic tokens per topic topic + + + betasum beta sum + = + 
			 displaycount display count + beta / tokenspertopic tokens per topic topic + betasum beta sum 


			 }
			*/ note that we've been absorbing alphasum alpha sum + doclength doc length into the normalizing constant the marginal probability needs this term so we stick it back in out normalizer + alphasum alpha sum + + + tokenssofar tokens so far 	make 	 make sure it actually gets set betatopiccount++ beta topic count++ smoothingonlycount++ smoothing only count++ todo t o d o is this appropriate put that new topic into the counts if this is a new topic for this document add the topic to the dense index first find the point where we should insert the new topic by going to the end which is the only reason we're keeping track of the number of non zero topics and working backwards 	update the coefficients for the non zero topics out type + \t + newtopic new topic + \t + loglikelihood log likelihood 	clean 	 clean up our mess reset the coefficients to values with only 	smoothing the next doc will update its own non zero topics 