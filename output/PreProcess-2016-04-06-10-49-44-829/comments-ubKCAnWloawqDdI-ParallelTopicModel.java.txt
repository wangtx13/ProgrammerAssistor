the training instances and their topic assignments the alphabet for the input data the alphabet for the topics number of topics to be fit these values are used to encode type/topic counts as count/topic pairs in a single dirichlet alpha alpha is the distribution over topics prior on per topic multinomial distribution over words indexed by <feature index topic index> indexed by <topic index> for dirichlet estimation histogram of document sizes histogram of document/topic counts indexed by <topic index sequence position index> the number of times each type appears in the corpus the max over typetotals type totals used for beta optimization exact power of 2 otherwise add an extra bit make sure we always have at least one sample before optimizing hyperparameters get the total number of occurrences of each word type typetotals type totals = new numtypes num types allocate enough space so that we never have to worry about overflows either the number of topics or the number of times the type occurs skip some lines starting with # that describe the format and specify hyperparameters clear the topic totals clear the type/topic counts only looking at the entries before the first 0 entry the format for these arrays is the topic in the rightmost bits the count in the remaining left bits since the count is in the high bits sorting desc by the numeric value of the guarantees that higher counts will be before the lower counts start by assuming that the is either empty or is in sorted descending order here we are only adding counts so if we find an existing location with the topic we only need to ensure that it is not larger than its left neighbor new value is 1 so we don't have to worry about sorting except by topic suffix which doesn't matter now ensure that the is still sorted by bubbling this value up clear the topic totals clear the type/topic counts only looking at the entries before the first 0 entry handle the total tokens per topic now handle the individual type topic counts here the source is the individual thread counts and the target is the global counts now ensure that the is still sorted by bubbling this value up /* debuggging to ensure counts are being 
		 reconstructed correctly 

		for type = 0 type < numtypes num types type++ {
			
			int targetcounts target counts = typetopiccounts type topic counts type 
			
			int index = 0 
			int count = 0 
			while index < targetcounts target counts length 
				 targetcounts target counts index > 0 {
				count += targetcounts target counts index >> topicbits topic bits 
				index++ 
			}
			
			if count != typetotals type totals type {
				system {
				 err expected + typetotals type totals type + found + count 
			}
			
		}
		*/ first clear the sufficient statistic histograms for the symmetric we only need one count which i'm putting in the same data structure but for topic 0 all other topic histograms will be empty i'm duplicating this for loop which isn't the best thing but it means only checking whether we are symmetric or not numtopics num topics times instead of numtopics num topics * longest document length 			 ^ the only change first clear the sufficient statistic histograms the histogram starts at count 0 so if all of the tokens of the most frequent type were assigned to one topic we would need to store a maxtypecount max type count + 1 count now count the number of type/topic pairs that have each number of tokens figure out how large we need to make the observation lengths histogram now allocate it and populate it now publish the new value some docs may be missing at the end due to division if there is only one thread copy the typetopiccounts type topic counts arrays directly rather than allocating new memory if there is only one thread we can avoid communications overhead this switch informs the thread not to gather statistics for its portion of the data submit runnables to thread pool runnables thread run i'm getting some problems that look like a thread hasn't started yet when it is first polled so it appears to be finished this only occurs in very short corpora are all the threads done? logger info thread + thread + done? + runnables thread isfinished is finished out print + currenttimemillis current time millis iterationstart iteration start + out print + currenttimemillis current time millis iterationstart iteration start + arraycopy typetopiccounts type topic counts type 0 counts 0 counts length initialize the tree sets collect counts how many words should we report? some topics may have fewer than the default number of words with non zero weight print results for each topic get counts of phrases logger info phrase +sbs phrases now filled with counts now start printing the xml for gathering <term> and <phrase> output temporarily so that we can get topic title information before printing it to out for holding candidate topic titles print words consider top 20 individual words as candidate titles /*
			for type = 0 type < alphabet size type++ 
				probs type = this getcountfeaturetopic get count feature topic type ti / this getcounttokenspertopic get count tokens per topic ti 
			rankedfeaturevector 
			 ranked feature vector rfv = new rankedfeaturevector ranked feature vector alphabet probs 
			for ri = 0 ri < numwords num words ri++ {
				int fi = rfv getindexatrank get index at rank ri 
				pout 	 <term weight=\ +probs fi + \ count=\ +this getcountfeaturetopic get count feature topic fi ti + \ > +alphabet lookupobject lookup fi +	 </term> 
				if ri < 20 consider top 20 individual words as candidate titles
					titles add alphabet lookupobject lookup fi this getcountfeaturetopic get count feature topic fi ti 
			}
			*/ print phrases any phrase count less than 20 is simply unreliable prefer phrases with a factor of 100 select candidate titles don't add redundant titles probably not the most efficient way to do this loop over the tokens in the document counting the current topic assignments add the smoothing and normalize and normalize initialize the sorters with dummy values count up the tokens and normalize /*
				out print doc out print ' ' 
				out print source out print ' ' 
				out print pi out print ' ' 
				out print type out print ' ' 
				out print alphabet lookupobject lookup type out print ' ' 
				out print topic out 
				*/ the likelihood of the model is a combination of a dirichlet multinomial for the words in each topic and a dirichlet multinomial for the topics in each document the likelihood function of a dirichlet multinomial is 	 gamma sum_i alpha_i 	 prod_i gamma alpha_i + n_i 	prod_i gamma alpha_i 	 gamma sum_i alpha_i + n_i so the log likelihood is 	loggamma 	log gamma sum_i alpha_i loggamma log gamma sum_i alpha_i + n_i + 	 sum_i loggamma log gamma alpha_i + n_i loggamma log gamma alpha_i do the documents first subtract the count + parameter sum term add the parameter sum term and the topics count the number of type topic pairs that are not just loggamma log gamma beta loggamma log gamma beta reuse this as a pointer loggamma log gamma |v|*beta | v|*beta for every topic loggamma log gamma beta for all type/topic pairs with non zero count serialization 