does not currently handle instances that are labeled with distributions instead of a single label implements commandoption command option listproviding list providing xxx why does testmaximizable test maximizable fail when this variance is very small? note used to be 1 cpal c p a l cpal c p a l cpal c p a l /*
	public mcmaxenttrainer m c max ent trainer maximizer bygradient by gradient maximizer 
	{
	this maximizerbygradient maximizer by gradient = maximizer 
	this usinghyperbolicprior using hyperbolic prior = 
	}
	*/ cpal c p a l added this to do multiconditionaltraining multi conditional training xxx x x x since we maximize before using numiterations num iterations this doesn't work is that a bug? if so should the default numiterations num iterations be higher? cpal c p a l change the tolerance for large vocab experiments std is 0001 xxx x x x given the loop below this seems wrong 		boolean converged 	 	for i = 0 i < numiterations num iterations i++ { 			converged = maximizer maximize mt 1 			if converged 			 	break 			else if evaluator != 			 	if !evaluator evaluate mt getclassifier get classifier converged i mt getvalue get value 				 												 trainingset training set validationset validation set testset test set 				 	break 		} 		testmaximizable 		 test maximizable testvalueandgradient test value and gradient mt progess messages are on one line move on /*
	public classifier trainwithfeatureinduction train with feature induction instancelist instance list trainingdata training data 
	 instancelist instance list validationdata validation data 
	 instancelist instance list testingdata testing data 
	 classifierevaluating classifier evaluating evaluator 
	 mcmaxent m c max ent maxent 

	 totaliterations total iterations 
	 numiterationsbetweenfeatureinductions num iterations between feature inductions 
	 numfeatureinductions num feature inductions 
	 numfeaturesperfeatureinduction num features per feature induction 
	 gainname gain name {

		 xxx x x x this ought to be a parameter except that setting it to can
		 crash training jump too small 
		boolean saveparametersduringfi save during f i = 

		alphabet 

		 alphabet inputalphabet input alphabet = trainingdata training data getdataalphabet get data alphabet 
		alphabet 
		 alphabet outputalphabet output alphabet = trainingdata training data gettargetalphabet get target alphabet 

		if maxent == 
			maxent = new mcmaxent m c max ent trainingdata training data getpipe get pipe 
			 new 1+inputalphabet 1+input alphabet size * outputalphabet output alphabet size 

		int trainingiteration training iteration = 0 
		int numlabels num labels = outputalphabet output alphabet size 

		 initialize feature selection
		featureselection selection
		 feature selection globalfs global f s = trainingdata training data getfeatureselection get feature selection 
		if globalfs global f s == {
			 mask out all features some will be added later by featureinducer feature inducer inducefeaturesfor induce features for 
			globalfs 
			global f s = new featureselection feature selection trainingdata training data getdataalphabet get data alphabet 
			trainingdata 
			training data setfeatureselection set feature selection globalfs global f s 
		}
		if validationdata validation data != validationdata validation data setfeatureselection set feature selection globalfs global f s 
		if testingdata testing data != testingdata testing data setfeatureselection set feature selection globalfs global f s 
		maxent = new mcmaxent m c max ent maxent getinstancepipe get instance pipe maxent getparameters get globalfs global f s 

		 run feature induction
		for featureinductioniteration feature induction iteration = 0 
		 featureinductioniteration feature induction iteration < numfeatureinductions num feature inductions 
		 featureinductioniteration++ feature induction iteration++ {

			 print out some feature information
			logger info feature induction iteration +featureinductioniteration +feature induction iteration 

			 train the model a little bit we don't care whether it converges we
			 execute all feature induction iterations no matter what 
			if featureinductioniteration feature induction iteration != 0 {
				 don't train until we have added some features
				setnumiterations features
				set num iterations numiterationsbetweenfeatureinductions num iterations between feature inductions 
				maxent = mcmaxent m c max ent this train trainingdata training data validationdata validation data testingdata testing data evaluator 
				 maxent 
			}
			trainingiteration 
			}
			training iteration += numiterationsbetweenfeatureinductions num iterations between feature inductions 

			logger info starting feature induction with + 1+inputalphabet 1+input alphabet size +
			 features over +numlabels+ +num labels+ labels 

			 create the list of tokens
			instancelist tokens
			 instance list errorinstances instances = new instancelist instance list trainingdata training data getdataalphabet get data alphabet 
			 trainingdata training data gettargetalphabet get target alphabet 

			 this errorinstances instances featureselection feature selection will get examined by featureinducer feature inducer 
			 so it can know how to add new singleton features
			errorinstances features
			error instances setfeatureselection set feature selection globalfs global f s 
			list 
			 list errorlabelvectors label vectors = new arraylist list these are length 1 vectors
			for i = 0 i < trainingdata training data size i++ {
				instance {
				 instance instance = trainingdata training data get i 
				featurevector 
				 feature vector inputvector input vector = featurevector feature vector instance getdata get data 
				label 
				 label truelabel label = label instance gettarget get target 

				 having trained using just the current features see how we classify
				 the training data now 
				classification 
				 classification classification = maxent classify instance 
				if !classification bestlabeliscorrect best label is correct {
					errorinstances {
					error instances add inputvector input vector truelabel label 
					errorlabelvectors 
					error label vectors add classification getlabelvector get label vector 
				}
			}
			logger info instance list size = +errorinstances +error instances size 
			int s = errorlabelvectors label vectors size 

			labelvector 

			 label vector lvs = new labelvector label vector s 
			for i = 0 i < s i++ {
				lvs i = labelvector label vector errorlabelvectors label vectors get i 
			}

			rankedfeaturevector 
			}

			 ranked feature vector factory gainfactory gain factory = 
			if gainname gain name equals exp_gain 
				gainfactory 
				gain factory = new expgain exp gain factory lvs gaussianpriorvariance gaussian prior variance 
			else if gainname gain name equals gradient_gain 
				gainfactory 
				gain factory =	new gradientgain gradient gain factory lvs 
			else if gainname gain name equals information_gain 
				gainfactory 
				gain factory =	new infogain info gain factory 
			else
				throw new illegalargumentexception illegal argument unsupported gain name +gainname +gain name 

			featureinducer 

			 feature inducer klfi =
			 new featureinducer feature inducer gainfactory gain factory 
			 errorinstances instances 
			 numfeaturesperfeatureinduction num features per feature induction 
			 2*numfeaturesperfeatureinduction 2*num features per feature induction 
			 2*numfeaturesperfeatureinduction 2*num features per feature induction 

			 note that this adds features globally but not on a per transition basis
			klfi inducefeaturesfor induce features for trainingdata training data 
			if testingdata testing data != klfi inducefeaturesfor induce features for testingdata testing data 
			logger info mcmaxent m c max ent featureselection feature selection now includes +globalfs +global f s cardinality + features 
			klfi = 

			double newparameters new = new 1+inputalphabet 1+input alphabet size * outputalphabet output alphabet size 

			 xxx x x x executing this block often causes an during training i don't know why 
			if saveparametersduringfi save during f i {
				 keep current parameter values
				 xxx x x x this relies on the detail that the most recent features
				 added to an alphabet get the highest indices 

				 count per output label
				int oldparamcount old count = maxent length / outputalphabet output alphabet size 
				int newparamcount new count = 1+inputalphabet 1+input alphabet size 
				 copy params into the proper locations
				for i=0 i<outputalphabet i<output alphabet size i++ {
					system {
					 arraycopy maxent i*oldparamcount i*old count 
					 newparameters new i*newparamcount i*new count 
					 oldparamcount old count 
				}
				for i=0 i<oldparamcount i<old count i++ 
					if maxent i != newparameters new i {
						system {
						 out maxent i + +newparameters +new i 
						system 
						 exit 0 
					}
			}

			maxent = newparameters new 
			maxent defaultfeatureindex default feature index = inputalphabet input alphabet size 
		}

		 finished feature induction
		logger info ended with +globalfs +global f s cardinality + features 
		setnumiterations 
		set num iterations totaliterations total iterations trainingiteration training iteration 
		return this train trainingdata training data validationdata validation data testingdata testing data 
		 evaluator maxent 
	}
	*/ xxx x x x should these really be public? why? 	public getiterations get iterations {return maximizerbygradient maximizer by gradient getiterations get iterations } 	+ +maximizerclass +maximizer getname get name + a inner that wraps up a mcmaxent m c max ent classifier and its training data the result is a maximize maximizable function the expectations are temporarily stored in the cachedgradient cached gradient just for clarity don't fd stopgrowth stop growth because someone might want to do feature induction add one feature for the default feature add the default feature index to the selection xxx later change this to allow both to be set but select which one to use by a flag? initialize the constraints logger fine instance +ii+ labeling= +labeling the 2* below is because there is one copy for the p y|x and another for the p x|y for the default feature whose weight is 1 0 only p y|x uses the default feature p x|y doesn't use it the default feature value is 1 0 testmaximizable test maximizable testvalueandgradientcurrentparameters test value and gradient current this log probability of the training labels we'll store the expectation values in cachedgradient cached gradient for now incorporate likelihood of data out i now +inputalphabet +input alphabet size + regular features ii = 0 normalize the to be per multinomials todo t o d o strongly consider some smoothing here what happens when all are zero? oh this should be no problem because exp 0 == 1 out l now +inputalphabet +input alphabet size + regular features 						continue cpal c p a l this is a loop over classes and their scores we compute the gradient by taking the dot product of the feature value and the probability of the cpal c p a l accumulating the current classifiers expectation of the feature vector counts for this label current classifier has expectation over label not over feature vector cpal c p a l if we wish to do multiconditional training we need another term for this accumulated expectation need something analogous to this this theclassifier the classifier getclassificationscores get classification scores instance scores this theclassifier the classifier getfeaturedistributions get feature distributions instance note is the label for this instance get the sum of the feature vector which is the number of counts for the document if we use that as input cpal c p a l get the additional term for the value of our log probability this computation amounts to the dot product of the feature vector and the probability vector cpal c p a l get the model expectation over features for the given if numfeatures*li num features*li + fi != 0 { matrixops matrix ops rowplusequals row plus equals cachedgradient cached gradient numfeatures num features fv } logger info expectations cachedgradient cached gradient print incorporate prior on maximize m a x i m i z e not n o t minimize m i n i m i z e cpal c p a l first get value then gradient gradient is constraint expectation parameters/gaussianpriorvariance parameters/gaussian prior variance this will fill in the cachedgradient cached gradient with the expectation cachedgradient cached gradient contains the negative expectations expectations are model expectations and constraints are empirical expectations cpal c p a l we need a second copy of the constraints actually we only want this for the feature values i've moved this up into getvalue get value if usingmulticonditionaltraining using multi conditional training { matrixops matrix ops plusequals plus equals cachedgradient cached gradient constraints } incorporate prior on a parameter may be set to infinity by an external user we set gradient to 0 because the parameter's value can never change anyway and it will mess up future calculations on the matrix such as norm set to zero all the gradient dimensions that are not among the selected features 