note that the current size of the label alphabet can be larger than it was at the time of training we are careful here to correctly handle those labels here for example we assume the log prior probability of those classes is minus infinity make sure the feature vector's feature dictionary matches what we are expecting from our data pipe and thus our notion of feature probabilities set the scores according to the feature weights and per probabilities guard against dataalphabet data alphabet or target alphabet growing can happen if classifying a never before seen feature ignore these get the scores in the range near zero where exp is more accurate exponentiate and normalize create and a classification err label = \n +labeling err predicted = \n +predicted err print +labelweight +label weight err label = \n +labeling err predicted = \n +predicted err print +labelweight +label weight serialization serialversionuid serial uid is overriden to prevent innocuous changes in this from making the serialization mechanism think the external format has changed write prior for each write of conditional probability estimates 