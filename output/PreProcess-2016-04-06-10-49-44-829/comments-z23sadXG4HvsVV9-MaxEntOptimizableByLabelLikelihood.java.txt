xxx why does testmaximizable test maximizable fail when this variance is very small? the expectations are temporarily stored in the cachedgradient cached gradient just for clarity don't fd stopgrowth stop growth because someone might want to do feature induction add one feature for the default feature add the default feature index to the selection xxx later change this to allow both to be set but select which one to use by a flag? initialize the constraints logger fine instance +ii+ labeling= +labeling for the default feature whose weight is 1 0 testmaximizable test maximizable testvalueandgradientcurrentparameters test value and gradient current this log probability of the training labels we'll store the expectation values in cachedgradient cached gradient for now incorporate likelihood of data out l now +inputalphabet +input alphabet size + regular features 					continue logger info expectations cachedgradient cached gradient print incorporate prior on maximize m a x i m i z e not n o t minimize m i n i m i z e gradient is constraint expectation parameters/gaussianpriorvariance parameters/gaussian prior variance this will fill in the cachedgradient cached gradient with the expectation incorporate prior on a parameter may be set to infinity by an external user we set gradient to 0 because the parameter's value can never change anyway and it will mess up future calculations on the matrix such as norm set to zero all the gradient dimensions that are not among the selected features out maxenttrainer max ent trainer gradient infinity norm = +matrixops + matrix ops infinitynorm infinity norm cachedgradient cached gradient xxx x x x should these really be public? why? 	public getiterations get iterations {return maximizerbygradient maximizer by gradient getiterations get iterations } 