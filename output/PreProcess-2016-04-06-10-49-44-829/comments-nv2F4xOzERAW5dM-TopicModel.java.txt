begin by importing documents from text to feature sequences pipes lowercase tokenize remove stopwords map to features data label name fields create a model with 100 topics alpha_t = 0 01 beta_w = 0 01 note that the first parameter is passed as the sum over topics while the second is use two parallel samplers which each look at one half the corpus and combine statistics after every iteration run the model for 50 iterations and stop this is for testing only for real applications use 1000 to 2000 iterations show the words and topics in the first instance the data alphabet maps word ids to strings estimate the topic distribution of the first instance given the current gibbs state get an of sorted sets of word id/count pairs show top 5 words in topics with proportions for the first document create a new instance with high probability of topic 0 create a new instance named test instance with empty target and source fields 