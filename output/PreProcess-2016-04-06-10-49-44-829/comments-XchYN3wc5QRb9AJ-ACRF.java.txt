accessors todo t o d o make an implement with loglineartemplate log linear template fixedtemplate fixed template handle default weights and regular weights checkcliquesizeconsistent check clique size consistent training debug build this bitsets that tell us what weights occur in the data we can allocate default weights now use those to allocate the sparsevectors sparse vectors create a sparse vector with the allowable indices specified in advance 					system 					 out acrf a c r f +this+ +i+ has index +indices j assumes weights already initialized add debugging marker debugging function 				system 				 out unrolledclique unrolled clique +clique 				system 				 out fv f v +clique fv 					system 					 out weights +idx+ +w 					w print i hate serialization 8830720632081401678l 8830720632081401678 l for potentials that have no weights but that know how te construct a potential template that generated this clique features for the clique factor compute for this clique if cachegraphs cache graphs this change in this varset's var set's factor since last grad call todo t o d o 	public unrolledgraph unrolled graph factorgraph factor graph 	implements compactible output guesses how much cache the undirected model should have space for setupgraph setup graph sigh accessors xxx these should be refactor to undirectedmodel undirected model and automatically add evidencepotentials evidence potentials convenience computes the residual of each factor without actually changing this unrolled graph compute the map m a p assignment /* vectors that contain the counts of features observed in the
 training data maps
  maps
 clique template x feature number => count
 */ /* vectors that contain the expected value over the
 * labels of all the features have seen the training data
 * but not the training labels 
 */ /* initialize constraints and expectations 
		 * to have the same dimensions as weights but to
		 * be all zero 
		 */ do the defaults first and now the others /* allocate for weights constraints and expectations */ /*
	if cacheunrolledgraphs cache unrolled graphs {
	unrolledgraphs {
	unrolled graphs = new unrolledgraph unrolled graph numinstances num instances 
	}
*/ /* not tested
	 maximizabledcrf maximizable d c r f maximizableacrf maximizable a c r f maxable instancelist instance list ilist 
	 {
	 logger finest initializing maximizableacrf maximizable a c r f 

	 this traindata train data = ilist 
	 initconstraintsexpectations init constraints expectations 
	 constraints = maxable constraints these can be shared
			
	 numinstances num instances = traindata train data size 
	 these must occur after initweights init weights 
	 this numparameters num = numweights num weights 
	 cachedgradient cached gradient = new numparameters num 

	 cachedvaluestale cached value stale = cachedgradientstale cached gradient stale = 

	 if cacheunrolledgraphs cache unrolled graphs {
	 unrolledgraphs unrolled graphs = new unrolledgraph unrolled graph numinstances num instances 
	 }

	 }
*/ /* negate initialvalue initial value and finalvalue value because the are in
 * terms of weights not values 
 */ functions for unit tests to get constraints and expectations i'm too lazy to make a deep copy callers should not modify these /*
 if savenum++ save num++ % saveperiod save period == 0 {
 out saving acrf a c r f 
 acrf a c r f this writeweights write weights weightfile weight 
 out done 
 }
 */ /* instance values must either always or never be included in
 * the total values we can't just sometimes skip a value
 * because it is infinite that off the total values 
 * we only allow an instance to have infinite value if it happens
 * from the start we don't compute the value for the instance
 * after the first round if any other instance has infinite
 * value after that it is an */ /* we could initialize bitset with one slot for every
 * instance but it is *probably* cheaper not to taking the
 * time hit to allocate the space if a bit becomes
 * necessary */ /* clear the sufficient statistics that we are about to fill */ /* fill in expectations for each instance */ /* compute marginals for each clique */ happens if all nodes are pruned 				unrolled dump /* save the expected value of each feature for when we
 compute the gradient */ /* add in the joint prob of the labeling */ /*
						printdebuginfo /*
						print debug info unrolled 
						throw new illegalstateexception
							  illegal state exception
							 instance + instance getname get name + used to have non infinite 
							 + value but now it has infinite value 
*/ /*					throw new illegalstateexception
						  illegal state exception
						 value is nan na n in acrf a c r f getvalue get value instance +i 
*/ /* incorporate gaussian prior on this means
 that for each weight we will add w^2 / 2 * variance to the
 log probability */ /* this will fill in the expectations */ /* index into current element of cachedgradient cached gradient */ first do gradient wrt defaultweights default weights now do other weights computed below /* a parameter may be set to infinity by an external user 
 * we set gradient to 0 because the parameter's value can
 * never change anyway and it will mess up future calculations
 * on the matrix */ reportgradient report gradient only useful for debugging for each assigment to the clique note that we get the assignmentiterator assignment iterator from the factor rather than the clique because the factor knows about any potential sparsity /* also note that we use assnit assn it indexofcurrentassn index of current assn this assumes that the ordering of variables in the
 * varset var set by lookupmargianl lookup margianl is consistent between all calls to the this is a somewhat brittle
 * assumption but i don't see how to relax it without being terribly inefficient */ maximizableacrf maximizable a c r f printing functions 				dumpassnforclique 				dump assn for clique assn clique templates convenient for constructing acrfs a c r fs i hate serialization 2113750667182393436l 2113750667182393436 l acrf a c r f 