serializable todo t o d o needs to be done? xxx why does testmaximizable test maximizable fail when this variance is very small? default_gaussian_prior_variance = 1 the expectations are temporarily stored in the cachedgradient cached gradient just for clarity don't fd stopgrowth stop growth because someone might want to do feature induction add one feature for the default feature add the default feature index to the selection xxx later change this to allow both to be set but select which one to use by a flag? initialize the constraints logger fine instance +ii+ labeling= +labeling here is the difference between this and the single label rather than only picking out the best index loop over all label indices for the default feature whose weight is 1 0 we'll store the expectation values in cachedgradient cached gradient for now incorporate likelihood of data out l now +inputalphabet +input alphabet size + regular features loop added by limin yao 					continue the model expectation? added by limin yao logger info expectations cachedgradient cached gradient print incorporate prior on maximize m a x i m i z e not n o t minimize m i n i m i z e gradient is constraint expectation parameters/gaussianpriorvariance parameters/gaussian prior variance this will fill in the cachedgradient cached gradient with the expectation incorporate prior on a parameter may be set to infinity by an external user we set gradient to 0 because the parameter's value can never change anyway and it will mess up future calculations on the matrix such as norm set to zero all the gradient dimensions that are not among the selected features out maxenttrainer max ent trainer gradient infinity norm = +matrixops + matrix ops infinitynorm infinity norm cachedgradient cached gradient xxx x x x should these really be public? why? 	public getiterations get iterations {return maximizerbygradient maximizer by gradient getiterations get iterations } 