the test instances and their topic assignments todo t o d o auto generated constructor stub todo t o d o auto generated constructor stub todo t o d o auto generated constructor stub todo t o d o auto generated constructor stub first training a topic model on training data inference on test data count typetopiccounts type topic counts re sampling on all data initialize test initial sampling on testdata this not yet obeying its last argument and must be for this to work sampletopicsforonedoc sample topics for one doc featuresequence feature sequence instance getdata get data topicsequence topic sequence construct test loop todo t o d o called by inferenceall inference all using unseen words in testdata todo t o d o auto generated stub 		populate topic counts iterate over the positions words in the document remove this token from all counts build a distribution over topics for this token /doclen /doc len 1+talpha 1+t alpha is constant across all topics sample a topic assignment from this distribution put that new topic into the counts what do we have typetopiccounts type topic counts tokenspertopic tokens per topic topic sequence of training and test data re gibbs sampling on all data inference on testdata one problem is how to deal with unseen words unseen words is in the alphabet but typetopicscount type topics count entry is added by limin yao initialize test initial sampling on testdata this not yet obeying its last argument and must be for this to work sampletopicsforonedoc sample topics for one doc featuresequence feature sequence instance getdata get data topicsequence topic sequence /*	if typetopiccounts type topic counts type size != 0 {
						topics i = r nextint next numtopics num topics 
					} else {
						topics i = 1 for unseen words
					}*/ construct test include sufficient statistics for this one doc add count on new data to n k w and n k * pay attention to unseen words type seen in training loop todo t o d o todo t o d o auto generated stub 		populate topic counts iterate over the positions words in the document remove this token from all counts build a distribution over topics for this token /doclen /doc len 1+talpha 1+t alpha is constant across all topics sample a topic assignment from this distribution put that new topic into the counts inference 3 for each doc for each iteration for each word compare against inference that is method2 for each iter for each doc for each word initialize test initial sampling on testdata this not yet obeying its last argument and must be for this to work sampletopicsforonedoc sample topics for one doc featuresequence feature sequence instance getdata get data topicsequence topic sequence /*	if typetopiccounts type topic counts type size != 0 {
						topics i = r nextint next numtopics num topics 
						typetopiccounts 
						type topic counts type adjustorputvalue adjust or put value topics i 1 1 
						tokenspertopic 
						tokens per topic topics i ++ 
					} else {
						topics i = 1 for unseen words
					}*/ construct test loop todo t o d o initialize test initial sampling on testdata this not yet obeying its last argument and must be for this to work sampletopicsforonedoc sample topics for one doc featuresequence feature sequence instance getdata get data topicsequence topic sequence construct test include sufficient statistics for this one doc add count on new data to n k w and n k * pay attention to unseen words type seen in training loop todo t o d o sampling with known theta from maxent todo t o d o auto generated stub iterate over the positions words in the document build a distribution over topics for this token /doclen /doc len 1+talpha 1+t alpha is constant across all topics sample a topic assignment from this distribution put that new topic into the counts print human readable doc topic matrix for further ir i r use n t|d +alpha t / doclen doc len + alphasum alpha sum print topic word matrix for further ir i r use initialize the sorters with dummy values count up the tokens and normalize and smooth by dirichlet prior alpha 