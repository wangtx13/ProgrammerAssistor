analogous to a cc mallet classify classification not actually constructed by model fitting but could be added for test documents maintainable serialization the training instances and their topic assignments the alphabet for the input data the alphabet for the topics number of topics to be fit dirichlet alpha alpha is the distribution over topics prior on per topic multinomial distribution over words instance list for empirical likelihood calculation an to put the topic counts for the current document initialized locally below defined here to avoid garbage collection overhead indexed by <document index topic index> indexed by <feature index topic index> indexed by <topic index> for dirichlet estimation histogram of document sizes histogram of document/topic counts indexed by <topic index sequence position index> was 50 was 200 was 10 	 was 50 was 50 can be safely called multiple times this will complain if it can't handle the situation todo t o d o akm a k m july 18 why wasn't the next line there previously? this typetopiccounts type topic counts = newtypetopiccounts new type topic counts else nothing changed nothing to be done this not yet obeying its last argument and must be for this to work include sufficient statistics for this one doc initialize the smoothing only sampling bucket initialize the cached coefficients using only smoothing /*
			 if outputmodelinterval output model interval != 0 iterations % outputmodelinterval output model interval == 0 {
			 this write new outputmodelfilename+' output model filename+' '+iterations 
			 }
			*/ todo t o d o this condition should also check that we have more than one sample to work with here the number of samples actually obtained is not yet tracked loop over every document in the corpus todo t o d o consider beginning by sub sampling? out topictermcount topic term count + \t + betatopiccount beta topic count + \t + smoothingonlycount smoothing only count populate topic counts iterate over the tokens words in the document remove this token from all counts build a distribution over topics for this token todo t o d o yipes memory allocation in the inner loop! but note that keys and getvalues get values is doing this too sample a topic assignment from this distribution put that new topic into the counts update the document topic count histogram 	for dirichlet estimation /* currently ignored */ 		populate topic counts 		initialize 		 initialize the topic count/beta sampling bucket 			initialize the normalization constant for the b * n_{t|d} term 			update the coefficients for the non zero topics 	iterate 	 iterate over the positions words in the document 	remove 	 remove this token from all counts note that we actually want to remove the key if it goes to zero not set it to 0 				 alpha topic + localtopiccounts local topic counts get topic * 				topictermvalues 				topic term values i / 				 tokenspertopic tokens per topic topic + betasum beta sum 				note 				 note i tried only doing this next bit if 				score > 0 but it didn't make any difference 				at least in the first few iterations 				topictermindices 				topic term indices i = topic 			indicate that this is the last topic 			topictermindices 			topic term indices i = 1 			make 			 make sure it actually gets set topictermcount++ topic term count++ betatopiccount++ beta topic count++ smoothingonlycount++ smoothing only count++ todo t o d o is this appropriate throw new illegalstateexception illegal state ldahyper l d a hyper new topic not sampled assert newtopic new topic != 1 			put 			 put that new topic into the counts 			update the coefficients for the non zero topics 		clean 		 clean up our mess reset the coefficients to values with only 		smoothing the next doc will update its own non zero topics 			update 			 update the document topic count histogram 			for dirichlet estimation treeset tree set is ~70x faster than rankedfeaturevector ranked feature vector dm d m get counts of phrases out phrase +sbs phrases now filled with counts now start printing the xml for gathering <term> and <phrase> output temporarily so that we can get topic title information before printing it to out for holding candidate topic titles print words consider top 20 individual words as candidate titles print phrases out topic +ti out topic +ti+ numphrases= num phrases= +rfv numlocations num locations any phrase count less than 20 is simply unreliable prefer phrases with a factor of 100 select candidate titles don't add redundant titles initialize the sorters with dummy values count up the tokens and normalize turbo topics /*
	private corpuswordcounts corpus word counts {
		alphabet {
		 alphabet unigramalphabet unigram alphabet 
		featurecounter 
		 feature counter unigramcounts unigram counts = new featurecounter feature counter unigramalphabet unigram alphabet 
		public corpuswordcounts corpus word counts alphabet alphabet {
			unigramalphabet {
			unigram alphabet = alphabet 
		}
		private mylog x { x == 0 ? 1000000 0 math log x }
		 the likelihood ratio significance test
		private significancetest significance test thisunigramcount this unigram count nextunigramcount next unigram count nextbigramcount next bigram count nexttotalcount next total count mincount min count {
 if nextbigramcount next bigram count < mincount min count 1 0 
 assert nextunigramcount next unigram count >= nextbigramcount next bigram count 
 log_pi_vu = mylog nextbigramcount next bigram count mylog thisunigramcount this unigram count 
 log_pi_vnu = mylog nextunigramcount next unigram count nextbigramcount next bigram count mylog nexttotalcount next total count nextbigramcount next bigram count 
 log_pi_v_old = mylog nextunigramcount next unigram count mylog nexttotalcount next total count 
 log_1mp_v = mylog 1 math exp log_pi_vnu 
 log_1mp_vu = mylog 1 math exp log_pi_vu 
 2 * nextbigramcount next bigram count * log_pi_vu + 
 		 nextunigramcount next unigram count nextbigramcount next bigram count * log_pi_vnu 
 		nextunigramcount 		next unigram count * log_pi_v_old + 
 		 thisunigramcount this unigram count nextbigramcount next bigram count * log_1mp_vu log_1mp_v 
		}
		public significantbigrams significant bigrams word {
		}
	}
	*/ to work around a bug in trove? serialization instance lists /* this block will print out the best topics for each label

		idsorter label

		id sorter wp = new idsorter sorter numtypes num types 

		for topic = 0 topic < numtopics num topics topic++ {

		for type = 0 type < numtypes num types type++ {
		wp type = new idsorter sorter type typetopiccounts type topic counts type topic /
		tokenspertopic /
		tokens per topic topic 
		}
		arrays 
		}
		 arrays sort wp 

		stringbuffer 

		 buffer terms = new stringbuffer buffer 
		for i = 0 i < 8 i++ {
		terms append instances getdataalphabet get data alphabet lookupobject lookup wp i 
		terms append 
		}

		system 
		}

		 out terms 
		for label = 0 label < topiclabelcounts topic label counts topic length label++ {
		system {
		 out topiclabelcounts topic label counts topic label + \t +
		instances gettargetalphabet get target alphabet lookupobject lookup label 
		}
		system 
		}
		 out 
		}

		*/ convert to log probabilities adding this check since testing instances may have types not found in training instances as pointed out by steven bethard the likelihood of the model is a combination of a dirichlet multinomial for the words in each topic and a dirichlet multinomial for the topics in each document the likelihood function of a dirichlet multinomial is 	 gamma sum_i alpha_i 	 prod_i gamma alpha_i + n_i 	prod_i gamma alpha_i 	 gamma sum_i alpha_i + n_i so the log likelihood is 	loggamma 	log gamma sum_i alpha_i loggamma log gamma sum_i alpha_i + n_i + 	 sum_i loggamma log gamma alpha_i + n_i loggamma log gamma alpha_i do the documents first subtract the count + parameter sum term add the parameter sum term and the topics count the number of type topic pairs recommended to use mallet/bin/vectors2topics instead 