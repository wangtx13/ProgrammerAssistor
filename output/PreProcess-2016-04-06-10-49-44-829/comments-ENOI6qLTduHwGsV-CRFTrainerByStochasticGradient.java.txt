t is the decaying factor lambda is some regularization depending on the training set size and the gaussian prior best way to choose learning rate is to run training on a sample and set it to the rate that produces maximum increase in likelihood or accuracy then to be conservative just halve the learning rate in general eta = 1/ lambda*t where lambda=priorvariance*numtraininginstances lambda=prior variance*num training instances after an initial eta_0 is set t_0 = 1/ lambda*eta_0 after each training step eta = 1/ lambda* t+t_0 t=0 1 2 infinity was 10 akm 1/25/08 reset the conservative estimate for learning rate shuffle the indices todo t o d o add some way to train by batches of instances where the batch memberships are determined externally? or provide some easy for creating batches calculate parameter gradient given these instances constraints expectations change the a little by this difference obeying weightsfrozen weights frozen 