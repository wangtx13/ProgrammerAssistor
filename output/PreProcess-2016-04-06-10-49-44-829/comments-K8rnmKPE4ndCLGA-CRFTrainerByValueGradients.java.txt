gsc keep instead of classnames this will give more flexibility to the user to setup new crfoptimizable* c r f optimizable* and then pass them directly in the constructor so the crfoptimizable c r f optimizable inner no longer creates crfoptimizable* c r f optimizable* 	class 	 optimizablebyvaluegradientclasses optimizable by value gradient classes gsc removing these options the user ought to set the weights before creating the trainer 	boolean usesparseweights use sparse weights = 	 gsc 	boolean useunsupportedtrick use unsupported trick = various values from crf c r f acting as indicators of when we need to re calculate expectations and values to getvalue get value because weights' values changed re calculate to getvaluegradient get value gradient because weights' values changed gsc removing this because the user will call setweightsdimensionsasin set weights dimensions as in 	private cachedweightsstructurestamp cached weights structure stamp = 1 re allocate crf weights expectations constraints because new states transitions use mcrf trainingset training set to see when we need to re allocate crf weights expectations constraints because we are using a different traininglist training list than last time gsc number of times to reset the optimizer and continue training when the could not step in current direction occurs gsc gsc user should call setweightsdimensionsasin set weights dimensions as in before the optimizable and trainer are created 		if cachedweightsstructurestamp cached weights structure stamp != crf weightsstructurechangestamp weights structure change stamp { 				if usesparseweights use sparse weights 					crf setweightsdimensionasin set weights dimension as in trainingset training set useunsupportedtrick use unsupported trick 	 				else 					crf setweightsdimensiondensely set weights dimension densely 			ocrf = 			cachedweightsstructurestamp 			cached weights structure stamp = crf weightsstructurechangestamp weights structure change stamp 		} this will set this mcrf if necessary alternative opt = new conjugategradient conjugate gradient 0 001 this will set this mcrf if necessary this will set this opt if necessary gsc timing each iteration gsc resetting the optimizer for specified number of times reset the optimizer and get a new one 				logger info catching saying converged 				converged = gsc see comment in getoptimizablecrf get optimizable c r f 	public setusesparseweights set use sparse weights b { usesparseweights use sparse weights = b } 	public getusesparseweights get use sparse weights { usesparseweights use sparse weights } 	 gsc 	public setuseunsupportedtrick set use unsupported trick b { useunsupportedtrick use unsupported trick = b } 	public getuseunsupportedtrick get use unsupported trick { useunsupportedtrick use unsupported trick } gsc change max number of times the optimizer can be reset before throwing the could not step in current direction set up 		protected optimizablecrf optimizable c r f crf c r f crf instancelist instance list ilist 		{ 			 set up 			this crf = crf 			this trainingset training set = ilist 			cachedgradie 			cached gradie = new crf getnumfactors get num factors 			class 			 parametertypes parameter types = new {crf { c r f instancelist instance list class} 			for i = 0 i < optimizablebyvaluegradientclasses optimizable by value gradient classes length i++ { 				try {	 					constructor 					 constructor c = optimizablebyvaluegradientclasses optimizable by value gradient classes i getconstructor get constructor parametertypes parameter types 					opts i = optimizable bygradientvalue by gradient value c newinstance new instance crf ilist 				} catch e { throw new illegalstateexception illegal state couldn't contruct optimizable bygradientvalue by gradient value } 			} 			cachedvalueweightsstamp 			cached value weights stamp = 1 			cachedgradientweightsstamp 			cached gradient weights stamp = 1 		} todo t o d o move these implementations into crf c r f and put here stubs that call them! the cached value is not up to date it was calculated for a different set of crf c r f weights cachedvalue cached value is now no longer stale priorgradient prior gradient is parameter/gaussianpriorvariance parameter/gaussian prior variance gradient is constraint expectation + priorgradient prior gradient == expectation constraint priorgradient prior gradient gradient points up hill i e in the direction of higher value this will fill in the this expectation updating it if necessary serialization of maximizablecrf maximizable c r f serialization for crftrainerbyvaluegradient c r f trainer by value gradient /* need to check for pointers */ out writeint write defaultfeatureindex default feature index 		out writeint write cachedweightsstructurestamp cached weights structure stamp 		out writeboolean write usesparseweights use sparse weights defaultfeatureindex default feature index = in readint read 		usesparseweights 		use sparse weights = in readboolean read 