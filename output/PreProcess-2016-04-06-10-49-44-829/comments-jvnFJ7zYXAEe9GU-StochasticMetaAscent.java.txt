only initialize these if they are in case someone wants to optimize a few iterations at a time get current update value and gradient for the current batch the below was originally written for stochastic meta descent we are maximizing so we want ascent flip the signs on the gradient to make it point downhill update learning rates for individual for the first iteration this will just be the initial step since gradienttrace gradient trace will be all zeros adjust based on direction adjust gradient trace adjust gradient trace set new converge criteria from gradientascent gradient ascent and limitedmemorybfgs limited memory b f g s compute finite difference approximation of the hessian product adjust by eps * vector and recompute gradient restore old calculate hessian product 