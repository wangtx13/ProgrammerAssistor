accumulate target label counts and make sure the sum of each instance's target label is 1 calculate the base entropy info d and the the label distribution of the given instances maps feature index > hashtable and each table maps split point > info gain split ratio go through each feature's split points in ascending order sort instances on this feature's values iterate through the sorted instances accumulate the label weights for instances passing the test for this feature spilt point pair calculate the info gain of using this pair to split insts into those with value of feature <= p versus > p if this split point creates a partition with too few instances ignore it if all instances pass or fail this test it is useless calculate the entropy of instances passing and failing the test calculate gain d t the information gained by testing on this feature split point pair calculate split d t the split information calculate the gain ratio end loop through sorted instances end loop through features for each feature's split point with at least average gain get the maximum gain ratio and the associated split point using the info gain as tie breaker if all feature vectors are identical or no splits are worthy all 0s tie breaker 